# =============================================================================
# DOCKER COMPOSE - Stack RAG complète : Ollama + Qdrant + AnythingLLM
# =============================================================================
# Services :
#   1. ollama        → Serveur d'inférence LLM local (port 11434)
#   2. qdrant        → Base de données vectorielle (ports 6333/6334)
#   3. anythingllm   → Interface web RAG tout-en-un (port 3001)
#   4. ollama-init   → Service éphémère qui télécharge les modèles au démarrage
#
# Utilisation :
#   - CPU uniquement  : docker compose up -d
#   - Avec GPU NVIDIA : docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
# =============================================================================

volumes:
  ollama_data:
    name: ollama_data
  qdrant_data:
    name: qdrant_data
  anythingllm_data:
    name: anythingllm_data

networks:
  rag-network:
    name: rag-network
    driver: bridge

services:

  # ---------------------------------------------------------------------------
  # SERVICE 1 : OLLAMA - Serveur d'inférence LLM
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:${OLLAMA_VERSION:-latest}
    container_name: ollama
    restart: unless-stopped
    ports:
      - "${OLLAMA_PORT:-11434}:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_NUM_PARALLEL=${OLLAMA_NUM_PARALLEL:-1}
      - OLLAMA_KEEP_ALIVE=${OLLAMA_KEEP_ALIVE:-300}
      - OLLAMA_HOST=0.0.0.0
    networks:
      - rag-network

  # ---------------------------------------------------------------------------
  # SERVICE 2 : OLLAMA-INIT - Téléchargement automatique des modèles
  # ---------------------------------------------------------------------------
  ollama-init:
    image: curlimages/curl:latest
    container_name: ollama-init
    depends_on:
      ollama:
        condition: service_started
    volumes:
      - ./scripts/init-models.sh:/init-models.sh:ro
    entrypoint: ["/bin/sh", "-c", "sleep 30 && /bin/sh /init-models.sh"]
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    networks:
      - rag-network
    restart: "no"

  # ---------------------------------------------------------------------------
  # SERVICE 3 : QDRANT - Base de données vectorielle
  # ---------------------------------------------------------------------------
  qdrant:
    image: qdrant/qdrant:${QDRANT_VERSION:-latest}
    container_name: qdrant
    restart: unless-stopped
    ports:
      - "${QDRANT_REST_PORT:-6333}:6333"
      - "${QDRANT_GRPC_PORT:-6334}:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    networks:
      - rag-network
    # Pas d'authentification pour l'environnement local

  # ---------------------------------------------------------------------------
  # SERVICE 4 : ANYTHINGLLM - Interface web RAG tout-en-un
  # ---------------------------------------------------------------------------
  anythingllm:
    image: mintplexlabs/anythingllm:${ANYTHINGLLM_VERSION:-latest}
    container_name: anythingllm
    restart: unless-stopped
    cap_add:
      - SYS_ADMIN
    ports:
      - "${ANYTHINGLLM_PORT:-3001}:3001"
    volumes:
      - anythingllm_data:/app/server/storage
    depends_on:
      ollama:
        condition: service_started
      qdrant:
        condition: service_started
    environment:
      # Configuration LLM (Ollama)
      - LLM_PROVIDER=ollama
      - OLLAMA_BASE_PATH=http://ollama:11434
      - OLLAMA_MODEL_PREF=${DEFAULT_LLM_MODEL:-llama3.1:latest}
      - OLLAMA_MODEL_TOKEN_LIMIT=${MODEL_TOKEN_LIMIT:-8192}

      # Configuration Embedding (Ollama)
      - EMBEDDING_ENGINE=ollama
      - EMBEDDING_BASE_PATH=http://ollama:11434
      - EMBEDDING_MODEL_PREF=${DEFAULT_EMBEDDING_MODEL:-nomic-embed-text:latest}
      - EMBEDDING_MODEL_MAX_CHUNK_LENGTH=${EMBEDDING_CHUNK_LENGTH:-8192}

      # Configuration Vector Database (Qdrant)
      - VECTOR_DB=qdrant
      - QDRANT_ENDPOINT=http://qdrant:6333
      # PAS de clé API pour Qdrant en local

      # Sécurité AnythingLLM
      - AUTH_TOKEN=${ANYTHINGLLM_AUTH_TOKEN:-}
      - JWT_SECRET=${JWT_SECRET:-change-this-secret-in-production}

      # Stockage
      - STORAGE_DIR=/app/server/storage

      # Multi-utilisateurs (optionnel)
      - DISABLE_VIEW_CHAT_HISTORY=${DISABLE_VIEW_CHAT_HISTORY:-}
      - JWT_EXPIRY=${JWT_EXPIRY:-7d}

    networks:
      - rag-network
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3001/api/ping || exit 1"]
      start_period: 30s
      interval: 30s
      timeout: 10s
      retries: 5
