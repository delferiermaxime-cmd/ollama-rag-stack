# ğŸš€ Stack RAG Locale â€” Ollama + Qdrant + AnythingLLM

> **Stack complÃ¨te de RAG (Retrieval-Augmented Generation) auto-hÃ©bergÃ©e avec Docker Compose.**
> Discutez avec vos documents en utilisant des LLM locaux â€” aucune donnÃ©e ne quitte votre machine.

---

## ğŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#-vue-densemble)
2. [Architecture](#-architecture)
3. [PrÃ©requis](#-prÃ©requis)
4. [Installation â€” Linux](#-installation--linux)
5. [Installation â€” Windows](#-installation--windows)
6. [DÃ©ploiement](#-dÃ©ploiement)
7. [Configuration post-dÃ©ploiement](#-configuration-post-dÃ©ploiement)
8. [Multi-utilisateurs](#-multi-utilisateurs)
9. [Commandes utiles](#-commandes-utiles)
10. [Personnalisation](#-personnalisation)

> ğŸ“˜ Un guide de dÃ©pannage exhaustif est disponible dans **[TROUBLESHOOTING.md](TROUBLESHOOTING.md)**.

---

## ğŸ” Vue d'ensemble

Cette stack dÃ©ploie 3 services interconnectÃ©s :

| Service | RÃ´le | Port |
|---------|------|------|
| **Ollama** | Serveur d'infÃ©rence LLM local | `11434` |
| **Qdrant** | Base de donnÃ©es vectorielle | `6333` (REST) / `6334` (gRPC) |
| **AnythingLLM** | Interface web RAG tout-en-un | `3001` |

**ModÃ¨les prÃ©-installÃ©s automatiquement :**

| ModÃ¨le | Type | Taille approx. | Usage |
|--------|------|-----------------|-------|
| `llama3.1:latest` | LLM | ~4.7 Go | Conversation gÃ©nÃ©rale, RAG |
| `llama3:8b` | LLM | ~4.7 Go | Alternative stable |
| `glm-4.7-flash:latest` | LLM | ~3.0 Go | TÃ¢ches rapides |
| `qwen3-vl:8b` | LLM multimodal | ~5.0 Go | Texte + images |
| `nomic-embed-text:latest` | Embedding | ~274 Mo | Indexation documents (recommandÃ©) |
| `bge-m3:latest` | Embedding | ~1.2 Go | Multilingue dense+sparse |
| `bge-m3:567m` | Embedding | ~567 Mo | Version compacte |
| `embeddinggemma:300m` | Embedding | ~300 Mo | LÃ©ger et performant |

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Machine HÃ´te                     â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚   Ollama     â”‚  â”‚  Qdrant  â”‚  â”‚ AnythingLLM  â”‚   â”‚
â”‚  â”‚              â”‚  â”‚          â”‚  â”‚              â”‚   â”‚
â”‚  â”‚ LLM Inferenceâ”‚  â”‚ Vector DBâ”‚  â”‚ Web UI + RAG â”‚   â”‚
â”‚  â”‚ :11434       â”‚  â”‚ :6333    â”‚  â”‚ :3001        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  
â”‚         â”‚               â”‚               â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                         â”‚                           â”‚
â”‚                   [rag-network]                     â”‚
â”‚                   RÃ©seau Docker                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Flux de donnÃ©es RAG :**
1. L'utilisateur uploade un document dans AnythingLLM
2. AnythingLLM dÃ©coupe le document en chunks
3. Chaque chunk est transformÃ© en vecteur via Ollama (modÃ¨le d'embedding)
4. Les vecteurs sont stockÃ©s dans Qdrant
5. Quand l'utilisateur pose une question :
   - La question est vectorisÃ©e
   - Qdrant retrouve les chunks les plus similaires
   - Les chunks pertinents + la question sont envoyÃ©s au LLM
   - Le LLM gÃ©nÃ¨re une rÃ©ponse contextualisÃ©e

---

## âš™ PrÃ©requis

### Configuration minimale

| Ressource | Minimum | RecommandÃ© |
|-----------|---------|------------|
| RAM | 8 Go | 16 Go+ |
| Stockage | 30 Go | 60 Go+ |
| CPU | 4 cÅ“urs | 8 cÅ“urs+ |
| GPU (optionnel) | NVIDIA 6 Go VRAM | NVIDIA 8 Go+ VRAM |

### Logiciels requis

| Logiciel | Version min. | VÃ©rification |
|----------|-------------|--------------|
| Docker Engine | 20.10+ | `docker --version` |
| Docker Compose | V2+ | `docker compose version` |
| Git (optionnel) | 2.0+ | `git --version` |
| NVIDIA Driver (si GPU) | 525+ | `nvidia-smi` |
| NVIDIA Container Toolkit (si GPU, Linux) | 1.13+ | `nvidia-ctk --version` |

---

## ğŸ§ Installation sur Linux

### Ã‰tape 1 : Installer Docker

```bash
# === Ubuntu / Debian ===

# Mise Ã  jour des paquets
sudo apt update && sudo apt upgrade -y

# Installation des dÃ©pendances
sudo apt install -y ca-certificates curl gnupg

# Ajout de la clÃ© GPG officielle Docker
sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

# Ajout du dÃ©pÃ´t Docker
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(. /etc/os-release && echo "$VERSION_CODENAME") stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# Installation de Docker Engine + Compose
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Ajout de votre utilisateur au groupe docker (Ã©vite d'utiliser sudo)
sudo usermod -aG docker $USER

# IMPORTANT : DÃ©connectez-vous et reconnectez-vous pour que le groupe prenne effet
# Ou exÃ©cutez : newgrp docker

# VÃ©rification
docker --version
docker compose version
```

### Ã‰tape 2 (optionnel) : Installer le support GPU NVIDIA

```bash
# PrÃ©requis : les drivers NVIDIA doivent dÃ©jÃ  Ãªtre installÃ©s
# VÃ©rification : nvidia-smi doit afficher votre GPU

# Installation du NVIDIA Container Toolkit
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | \
  sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg

curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
  sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
  sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt update
sudo apt install -y nvidia-container-toolkit

# Configuration de Docker pour utiliser le runtime NVIDIA
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# VÃ©rification : doit afficher les infos GPU
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

### Ã‰tape 3 : DÃ©ployer la stack

Passez Ã  la section [DÃ©ploiement](#-dÃ©ploiement).

---

## ğŸªŸ Installation sur Windows

### Ã‰tape 1 : Installer Docker Desktop

1. **TÃ©lÃ©chargez Docker Desktop** depuis [docker.com/products/docker-desktop](https://www.docker.com/products/docker-desktop/)
2. **ExÃ©cutez l'installateur** et suivez les instructions
3. **Activez WSL 2** si demandÃ© (Docker Desktop le propose automatiquement)
4. **RedÃ©marrez** votre ordinateur si nÃ©cessaire
5. **Lancez Docker Desktop** depuis le menu DÃ©marrer

**VÃ©rification dans PowerShell :**

```powershell
docker --version
docker compose version
```

### Ã‰tape 2 (optionnel) : Activer le support GPU NVIDIA

> **PrÃ©requis :** Carte graphique NVIDIA avec drivers Ã  jour (525+)

1. Ouvrez **Docker Desktop** â†’ **Settings** (âš™ï¸)
2. Allez dans **Resources** â†’ **WSL Integration**
3. Activez l'intÃ©gration WSL pour votre distribution
4. Dans **General**, vÃ©rifiez que **"Use the WSL 2 based engine"** est cochÃ©
5. Appliquez et redÃ©marrez Docker Desktop

**VÃ©rification dans PowerShell :**

```powershell
# Doit afficher les infos de votre GPU
docker run --rm --gpus all nvidia/cuda:12.0.0-base-ubuntu22.04 nvidia-smi
```

> **Note Windows :** Le support GPU dans Docker Desktop pour Windows nÃ©cessite
> Windows 11 (ou Windows 10 21H2+) avec WSL 2 et les drivers NVIDIA 525+.

### Ã‰tape 3 : DÃ©ployer la stack

Passez Ã  la section [DÃ©ploiement](#-dÃ©ploiement).

---

## ğŸš€ DÃ©ploiement

### 1. RÃ©cupÃ©rer le projet

```bash
# Option A : Cloner avec Git
git clone https://github.com/delferiermaxime-cmd/ollama-rag-stack.git ollama-rag-stack
cd ollama-rag-stack

# Option B : TÃ©lÃ©charger et extraire manuellement
# Puis : cd ollama-rag-stack
```

### 2. Configurer l'environnement

```bash
# Copier le fichier d'exemple
cp env.example .env
cat .env (pour vÃ©rifier)

# Ã‰ditez .env selon vos besoins (optionnel, les valeurs par dÃ©faut fonctionnent)
# Linux : nano .env
# Windows PowerShell : notepad .env
```

### 3. Rendre les scripts exÃ©cutables (Linux uniquement)

```bash
chmod +x scripts/*.sh
```

### 4. Lancer la stack

```bash
# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  MODE CPU (sans GPU)                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
docker compose up -d

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘  MODE GPU NVIDIA                          â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
```

### 5. Suivre le tÃ©lÃ©chargement des modÃ¨les

Le premier lancement tÃ©lÃ©charge tous les modÃ¨les (~15-20 Go). Suivez la progression :

```bash
# Suivre les logs du service d'initialisation en temps rÃ©el
docker logs -f ollama-init

# Ou suivre tous les services
docker compose logs -f
```

> â± **Le premier dÃ©marrage peut prendre 15-60 minutes** selon votre connexion Internet.
> Les dÃ©marrages suivants seront quasi instantanÃ©s car les modÃ¨les sont persistÃ©s.

### 6. VÃ©rifier l'installation

```bash
# Linux
./scripts/check-health.sh

# Windows PowerShell
docker compose ps
curl http://localhost:11434/         # Ollama
curl http://localhost:6333/healthz   # Qdrant
curl http://localhost:3001/api/ping  # AnythingLLM
```

### 7. AccÃ©der aux services

| Service | URL |
|---------|-----|
| **AnythingLLM** (interface principale) | [http://localhost:3001](http://localhost:3001) |
| **Ollama API** | [http://localhost:11434](http://localhost:11434) |
| **Qdrant Dashboard** | [http://localhost:6333/dashboard](http://localhost:6333/dashboard) |

---

## ğŸ”§ Configuration post-dÃ©ploiement

### Configurer AnythingLLM (premier lancement)

1. Ouvrez [http://localhost:3001](http://localhost:3001) dans votre navigateur
2. Suivez l'assistant de configuration :
   - **LLM Provider** : SÃ©lectionnez `Ollama`
   - **Ollama URL** : `http://ollama:11434` (URL interne Docker)
   - **ModÃ¨le LLM** : Choisissez `llama3.1:latest` (ou un autre de la liste)
   - **Embedding Provider** : SÃ©lectionnez `Ollama`
   - **ModÃ¨le d'embedding** : Choisissez `nomic-embed-text:latest`
   - **Vector Database** : SÃ©lectionnez `Qdrant`
   - **Qdrant URL** : `http://qdrant:6333` (URL interne Docker)
3. CrÃ©ez un workspace et commencez Ã  uploader des documents !

### Tester Ollama directement

```bash
# Lister les modÃ¨les disponibles
curl http://localhost:11434/api/tags

# Tester une gÃ©nÃ©ration
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:latest",
  "prompt": "Bonjour ! Explique-moi le RAG en une phrase.",
  "stream": false
}'

# Tester un embedding
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text:latest",
  "prompt": "Ceci est un test d embedding"
}'
```

---

## ğŸ‘¥ Multi-Utilisateurs

AnythingLLM supporte nativement le mode multi-utilisateurs avec des rÃ´les.
Tous les utilisateurs partagent la mÃªme URL et la mÃªme base vectorielle Qdrant.
Les accÃ¨s sont contrÃ´lÃ©s par des rÃ´les assignÃ©s par l'administrateur.

### Activation

**Ã‰tape 1 :** Lancez la stack normalement (`docker compose up -d`)

**Ã‰tape 2 :** Ouvrez AnythingLLM â†’ **Settings** (âš™ï¸) â†’ **Security** â†’ **Enable Multi-User Mode**

**Ã‰tape 3 :** CrÃ©ez le compte administrateur (username + mot de passe)

> âš ï¸ **ATTENTION : Cette action est IRRÃ‰VERSIBLE.** Une fois le mode multi-user activÃ©, impossible de revenir en single-user.

**Ã‰tape 4 :** CrÃ©ez des utilisateurs via **Settings** â†’ **Users**

### RÃ´les disponibles

| RÃ´le | Workspaces | Documents | Settings systÃ¨me | Gestion users |
|------|-----------|-----------|-----------------|---------------|
| **Admin** | Tous | Tous | âœ… | âœ… |
| **Manager** | Tous | Tous | âŒ | âŒ |
| **Default** | AssignÃ©s uniquement | AssignÃ©s uniquement | âŒ | âŒ |

### Comment Ã§a marche avec Qdrant

Quand un document est embedÃ© dans un workspace, tous les utilisateurs ayant accÃ¨s Ã  ce workspace peuvent interroger ces documents via RAG. Les vecteurs sont stockÃ©s dans Qdrant dans des collections nommÃ©es automatiquement par workspace. Un utilisateur "Default" ne voit que les workspaces auxquels l'admin l'a assignÃ©.

### Configuration avancÃ©e : SSO Simple

Pour intÃ©grer AnythingLLM dans un systÃ¨me d'authentification existant :

```bash
# 1. Activez dans .env :
SIMPLE_SSO_ENABLED=enable

# 2. RedÃ©marrez :
docker compose restart anythingllm

# 3. GÃ©nÃ©rez un lien de connexion via l'API :
curl -X POST http://localhost:3001/api/v1/users/{user_id}/issue-auth-token \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json"
```

---

## ğŸ“ Commandes utiles

### Gestion des services

```bash
# DÃ©marrer la stack
docker compose up -d

# DÃ©marrer avec GPU
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d

# ArrÃªter (conserver les donnÃ©es)
docker compose stop

# RedÃ©marrer
docker compose restart

# ArrÃªter et supprimer les conteneurs (volumes conservÃ©s)
docker compose down

# TOUT supprimer (conteneurs + volumes + donnÃ©es)
docker compose down -v --remove-orphans
```

### Logs et dÃ©bogage

```bash
docker compose logs              # Tous les logs
docker compose logs -f           # Temps rÃ©el
docker compose logs -f ollama    # Un service spÃ©cifique
```

### Gestion des modÃ¨les Ollama

```bash
docker exec -it ollama ollama pull <modele>   # Ajouter
docker exec -it ollama ollama list            # Lister
docker exec -it ollama ollama rm <modele>     # Supprimer
docker exec -it ollama ollama show <modele>   # Infos
```

### Sauvegarde et restauration

```bash
# Sauvegarder
docker run --rm -v ollama_data:/data -v $(pwd)/backup:/backup \
  alpine tar czf /backup/ollama_backup.tar.gz -C /data .

docker run --rm -v qdrant_data:/data -v $(pwd)/backup:/backup \
  alpine tar czf /backup/qdrant_backup.tar.gz -C /data .

docker run --rm -v anythingllm_data:/data -v $(pwd)/backup:/backup \
  alpine tar czf /backup/anythingllm_backup.tar.gz -C /data .

# Restaurer
docker run --rm -v ollama_data:/data -v $(pwd)/backup:/backup \
  alpine sh -c "cd /data && tar xzf /backup/ollama_backup.tar.gz"
```

---

## ğŸ¨ Personnalisation

### Ajouter/supprimer des modÃ¨les

Ã‰ditez `scripts/init-models.sh` et modifiez la variable `MODELS`, puis relancez :

```bash
docker compose run --rm ollama-init
```

Parcourez la bibliothÃ¨que sur [ollama.com/library](https://ollama.com/library).

### Modifier les paramÃ¨tres Qdrant

Ã‰ditez `config/qdrant/config.yaml` puis : `docker compose restart qdrant`

### Exposer sur le rÃ©seau local

Les services Ã©coutent sur toutes les interfaces par dÃ©faut. Depuis une autre machine :
`http://<IP_DE_LA_MACHINE>:3001`

---

## ğŸ“ Structure du projet

```
ollama-rag-stack/
â”œâ”€â”€ docker-compose.yml          # Configuration principale (CPU)
â”œâ”€â”€ docker-compose.gpu.yml      # Override pour GPU NVIDIA
â”œâ”€â”€ .env.example                # Template des variables d'environnement
â”œâ”€â”€ .env                        # Votre configuration locale (crÃ©Ã© par vous)
â”œâ”€â”€ .gitignore                  # Fichiers exclus de Git
â”œâ”€â”€ README.md                   # Ce fichier
â”œâ”€â”€ TROUBLESHOOTING.md          # Guide de dÃ©pannage exhaustif
â”œâ”€â”€ config/
â”‚   â””â”€â”€ qdrant/
â”‚       â””â”€â”€ config.yaml         # Configuration avancÃ©e de Qdrant
â””â”€â”€ scripts/
    â”œâ”€â”€ init-models.sh          # TÃ©lÃ©chargement automatique des modÃ¨les
    â”œâ”€â”€ check-health.sh         # VÃ©rification de santÃ© de la stack
    â””â”€â”€ stop-and-clean.sh       # ArrÃªt et nettoyage
```

---

## ğŸ“„ Licence

Ce projet est fourni tel quel, libre d'utilisation et de modification.
Les composants utilisÃ©s ont leurs propres licences :
- Ollama : [MIT License](https://github.com/ollama/ollama/blob/main/LICENSE)
- Qdrant : [Apache 2.0](https://github.com/qdrant/qdrant/blob/master/LICENSE)
- AnythingLLM : [MIT License](https://github.com/Mintplex-Labs/anything-llm/blob/master/LICENSE)
